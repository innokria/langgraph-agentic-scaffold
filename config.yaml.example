# SpecialistHub Configuration Example
# Version: 2.1
#
# This file defines the structure of the agentic system. It is the single
# source of truth for wiring together providers, models, and specialists.
# Copy this file to `config.yaml` and customize it for your setup.

# ==============================================================================
# LLM PROVIDER CONFIGURATION
# ==============================================================================
# Define the available LLM providers. The keys here (e.g., 'lmstudio') are
# referenced by the specialists below.
# The application will read environment variables defined in .env for secrets.
#
# Supported providers: "lmstudio", "ollama", "gemini"
# ==============================================================================
llm_providers:
  lmstudio:
    # Uses the LMSTUDIO_BASE_URL from your .env file
    # Example: LMSTUDIO_BASE_URL=http://localhost:1234/v1
    provider: "lmstudio"
  ollama:
    # Uses OLLAMA_BASE_URL and OLLAMA_MODEL from your .env file
    # Example: OLLAMA_BASE_URL=http://localhost:11434
    provider: "ollama"
  gemini:
    # Uses GEMINI_API_KEY from your .env file
    provider: "gemini"

# ==============================================================================
# SPECIALIST CONFIGURATION
# ==============================================================================
# Define each specialist agent. The key for each specialist (e.g., 'router_specialist')
# must match the specialist's Python module name (router_specialist.py) and the
# name passed to super().__init__() in its constructor.
# ==============================================================================
specialists:
  # --- Orchestration & Planning ---
  router_specialist:
    prompt_file: "router_prompt.md"
    description: "The master router and planner. It analyzes the user's request and routes it to the appropriate specialist. It is the entry point of the graph."
    # This model should be capable of reliable tool/function calling.
    api_identifier: "gemini-2.5-flash"
    provider: "gemini"

  systems_architect:
    prompt_file: "systems_architect_prompt.md"
    description: "Analyzes a user's request and creates a high-level technical plan. Produces a 'system_plan' artifact. This is a good first step for complex tasks."
    api_identifier: "gemini-2.5-flash"
    provider: "gemini"

  # --- Core Capabilities ---
  web_builder:
    prompt_file: "web_builder_prompt.md"
    description: "Takes a 'system_plan' artifact and generates a self-contained HTML document based on it. Requires a 'system_plan' to be present in the state."
    api_identifier: "gemini-2.5-flash"
    provider: "gemini"

  file_specialist:
    prompt_file: "file_specialist_pydantic_prompt.md" # Use a prompt designed for Pydantic/JSON schema output.
    description: "A specialist that uses a Pydantic schema to interact with the filesystem (read, list)."
    # This model must be good at generating structured JSON.
    api_identifier: "gemini-2.5-flash"
    provider: "gemini"
    root_dir: "./workspace" # Defaults to a 'workspace' folder in the project root.

  # --- Data & Analysis ---
  data_extractor_specialist:
    prompt_file: "data_extractor_prompt.md"
    description: "Extracts structured JSON data from unstructured text based on a schema."
    api_identifier: "gemini-2.5-flash"
    provider: "gemini"

  sentiment_classifier_specialist:
    prompt_file: "sentiment_classifier_prompt.md"
    description: "Classifies the sentiment of a given text as positive, negative, or neutral."
    api_identifier: "gemini-2.5-flash"
    provider: "gemini"

  data_processor_specialist:
    # This is a procedural specialist and does not require an LLM.
    description: "A specialist that performs deterministic data processing tasks, like formatting or cleaning, without calling an LLM."

  text_analysis_specialist:
    api_identifier: "gemini-2.5-flash"
    provider: "gemini"
    prompt_file: "text_analysis_prompt.md"
    description: "Analyzes, summarizes, or extracts information from a block of text. Use this after text has been retrieved by another specialist."
    
  # --- General & Fallback ---
  prompt_specialist:
    prompt_file: "prompt_prompt.md"
    description: "A general-purpose specialist for direct Q&A and instruction following. Acts as a fallback."
    api_identifier: "gemini-2.5-flash"
    provider: "gemini"

  # --- Wrapped & External Specialists ---
  open_swe_specialist:
    type: wrapped
    source: "./external_agents/open_swe/run.py" # Relative path from project root
    class_name: "Agent" # The name of the class to instantiate from the source file.
    description: "A specialist that wraps the open-swe agent for software engineering tasks."

  archiver_specialist:
    description: "Summarizes the conversation and prepares the final report. This is the final step."
