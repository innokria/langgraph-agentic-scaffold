# SpecialistHub Configuration Example
# Version: 2.0
#
# This file defines the structure of the agentic system. It is the single
# source of truth for wiring together providers, models, and specialists.
# Copy this file to `config.yaml` and customize it for your setup.

# ==============================================================================
# LLM PROVIDER CONFIGURATION
# ==============================================================================
# Define the available LLM providers. The keys here (e.g., 'lmstudio') are
# referenced by the specialists below.
# The application will read environment variables defined in .env for secrets.
#
# Supported providers: "lmstudio", "ollama", "gemini"
# ==============================================================================
llm_providers:
  lmstudio:
    # Uses the LMSTUDIO_BASE_URL from your .env file
    # Example: LMSTUDIO_BASE_URL=http://localhost:1234/v1
    provider: "lmstudio"
  ollama:
    # Uses OLLAMA_BASE_URL and OLLAMA_MODEL from your .env file
    # Example: OLLAMA_BASE_URL=http://localhost:11434
    provider: "ollama"
  gemini:
    # Uses GEMINI_API_KEY from your .env file
    provider: "gemini"

# ==============================================================================
# SPECIALIST CONFIGURATION
# ==============================================================================
# Define each specialist agent. The key for each specialist (e.g., 'router_specialist')
# must match the specialist's Python module name (router_specialist.py) and the
# name passed to super().__init__() in its constructor.
# ==============================================================================
specialists:
  router_specialist:
    prompt_file: "router_specialist_prompt.md"
    description: "The master router. It analyzes the user's request and routes it to the appropriate specialist. It is the entry point of the graph."
    # This model should be capable of reliable tool/function calling.
    model: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF"
    provider: "lmstudio"

  data_extractor_specialist:
    prompt_file: "data_extractor_specialist_prompt.md"
    description: "Extracts structured JSON data from unstructured text based on a schema."
    # This model should be good at following JSON formatting instructions.
    model: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF"
    provider: "lmstudio"

  file_specialist:
    prompt_file: "file_specialist_prompt.md"
    description: "A specialist that uses tools to interact with the filesystem (read, write, list)."
    # This model must be capable of tool/function calling.
    model: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF"
    provider: "lmstudio"
    # The root directory this specialist is allowed to operate in.
    # For security, this should be a dedicated, non-sensitive directory.
    root_dir: "./workspace" # Defaults to a 'workspace' folder in the project root.

  prompt_specialist:
    prompt_file: "prompt_specialist_prompt.md"
    description: "A general-purpose specialist for direct Q&A and instruction following. Acts as a fallback."
    model: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF"
    provider: "lmstudio"

  systems_architect:
    prompt_file: "systems_architect_prompt.md"
    description: "Designs system components and generates structured data, like sequence diagrams in JSON format."
    model: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF"
    provider: "lmstudio"

  web_builder:
    prompt_file: "web_builder_prompt.md"
    description: "Takes structured data (JSON) and generates an HTML visualization."
    model: "lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF"
    provider: "lmstudio"

  data_processor_specialist:
    # This is a procedural specialist and does not require an LLM.
    # By omitting 'model' and 'provider', the AdapterFactory will assign a None adapter.
    # This demonstrates how to create non-AI nodes in the graph for deterministic tasks.
    description: "A specialist that performs deterministic data processing tasks, like formatting or cleaning, without calling an LLM."
